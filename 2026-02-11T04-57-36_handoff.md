# Handoff Summary — 2026-02-11T04-57-36

## Context

### 1) Previous Conversation

This session evolved from a simple “what are next tasks?” request into a long, phased execution program focused on improving evaluation quality for the font-matching pipeline.

Major milestones and decisions (chronological):

- Initial planning aligned to LightSpec progress and active decision records.
- Track A delegation executed (specimen/prompt/schema/uncertainty improvements), with user override to route vision-labeling work through specific models over time.
- Multiple rounds of model-vs-human alignment were run (spot checks, then full-set comparisons), with iterative UX/report artifact updates.
- Repeated refinements were requested by user, including:
  - changing micro-tell pair from `o0` to `O0`,
  - removing font-name leakage from model-visible prompts,
  - switching from spot checks to comprehensive full-set,
  - regenerating conflict-review HTML with different interaction modes,
  - adding/replacing tested models.
- Core evaluation direction shifted toward:
  - Agreement as primary optimization target,
  - strict visual QA for specimen rendering,
  - production-like policies using Specimen V3/V3.1, Prompt V3, and gating.
- Critical render defects were reported by user and delegated for fix:
  - right-side cropping,
  - vertical overlap.
  These were reported fixed and then manually spot-checked by delegate including the exact user-specified files.
- Three-week delegated roadmap executed:
  - Week 1: Prompt V3 vs V4 A/B on Gemini 3 Pro.
  - Week 2: Specimen V3.1 upgrade + mandatory visual QA + rerun.
  - Week 3: threshold calibration/tuning.
- Governance lock then requested and delegated:
  - single evaluation contract,
  - hard acceptance gates,
  - reproducibility + quota standardization,
  - LightSpec governance updates.

User preferences explicitly stated across session:

- Prioritize quality/validation over rushing to production.
- Agreement is primary metric preference (with tradeoff awareness).
- Cost is *not* a deciding factor when choosing best model.
- Strict intolerance for unacceptable specimens (crop/overlap/layout artifacts).
- Require manual visual checks on previously problematic files.
- Keep experimentation broad/creative, but quantified and reproducible.

---

### 2) Current Work

State right before this handoff request:

- Mode: orchestrator.
- Main objective immediately before handoff: complete governance upgrades (“Lock a single evaluat..., Add hard acceptance gates..., Standardize reprod... do all of these now.”).
- Completion phase status: **delegated completion reported** as done, including creation/updates to evaluation contract, validation checker, reproducibility docs, and LightSpec decision/progress updates.
- Confidence note: many implementation details were reported by delegates and summarized upstream; not every modified file was independently re-opened line-by-line in the parent thread after each delegate completion.

Estimated completion of user-requested scope to date:

- Experimental roadmap (Week 1–3): reported complete.
- Visual QA requirement on named specimen regressions: reported complete.
- Governance lock request: reported complete.

---

### 3) Key Technical Concepts

Project/environment conventions used in this session:

- App stack:
  - TypeScript 5.7.3 + Next.js 15.1.6 + React 19 + Tailwind 4.
- Evaluation stack:
  - Python scripts under `research/ab-eval/py/`.
- Storage/reporting:
  - JSON artifacts under `research/ab-eval/out/`.
  - Markdown reports under `research/ab-eval/`.
- Governance/SSoT:
  - LightSpec under `.lightspec/`.

Operational constraints repeatedly enforced:

- No font-name leakage in model-visible prompt payload.
- Compare on amended human SSoT export used as canonical evaluation source in later phases.
- Full-set comparisons expected to preserve canonical pair identity and denominator discipline.
- Visual QA manually required for specimen regressions.
- Provider routing constraints:
  - Gemini models via Gemini API key path.
  - Non-Gemini models primarily via OpenRouter.
- Quota/rate limit behavior is real and affected runs (notably Gemini 3 Pro 429 RESOURCE_EXHAUSTED episodes).

Performance-policy patterns surfaced:

- Strict gating improves precision, often hurts recall.
- Prompt changes can help one dimension while hurting Agreement.
- Specimen layout quality materially affects downstream model behavior.
- Non-binary labels (`2`) create accounting ambiguity unless normalized by explicit policy.

---

### 4) Relevant Files and Code (File System Impact Registry)

> Note: list is based on explicit read/edit/delegate completion reports in this conversation.

#### A) Frequently referenced governance and planning files

- `.lightspec/progress.md`
  - Updated repeatedly to track phases, week milestones, and governance lock.
- `.lightspec/font-search-rag.md`
  - Reported updated during governance lock cycle.
- `.lightspec/decisions/DEC-20260208-03-quality-first-experiment-plan.md`
  - Used as early decision anchor.
- `.lightspec/decisions/DEC-20260208-04-track-a-implementation.md`
  - Reported created for Track A implementation capture.
- `.lightspec/decisions/DEC-20260209-01-ai-vs-human-alignment-spotcheck.md`
  - Reported created for alignment spot-check.
- `.lightspec/decisions/DEC-20260209-01-fontclip-quantification-adoption.md`
  - Reported created for FontCLIP quantification.
- `.lightspec/decisions/DEC-20260209-02-specimen-v3-prompt-v3-adoption.md`
  - Reported updated/finalized during V3 policy promotion cycle.
- `.lightspec/decisions/DEC-20260211-01-evaluation-governance-lock.md`
  - Reported created for governance lock.

#### B) Core eval pipeline scripts reported created/updated

- `research/ab-eval/py/run_production_trial.py`
  - Became central production-like runner.
  - Reported additions over time:
    - model switchability,
    - prompt variant selection (V3/V4),
    - key cycling (`--keys-file`),
    - smoke limiter (`--max-fonts`),
    - output/cache control,
    - specimen directory switching (`--spec-dir`),
    - improved error surfacing (429/503).

- `research/ab-eval/py/render_specimen_v2.py`
  - Reported for V2 deterministic specimen generation.
  - Micro-tell pair update request applied (`O0` vs `o0`).

- `research/ab-eval/py/render_specimen_v3.py`
  - Reported updated to fix right-edge crop and vertical overlap using dynamic layout/wrapping/scaling.

- `research/ab-eval/py/render_specimen_v3_1.py`
  - Reported created for V3.1 with larger distinction blocks (`il1I0O` and expanded style identifiers).

- `research/ab-eval/py/run_spot_check_alignment.py`
  - Reported created for GPT-5.2 alignment spot checks.

- `research/ab-eval/py/run_spot_check_alignment_models.py`
  - Reported created/updated for multi-model spot checks and batching.

- `research/ab-eval/py/run_comprehensive_235b.py`
  - Reported created for full-set 235B run.

- `research/ab-eval/py/run_full_comparison.py`
  - Reported repeatedly enhanced:
    - no-name-bias behavior,
    - batched request strategy,
    - support for multiple model routes,
    - later Gemini API route support for Gemini 3 flash preview.

- `research/ab-eval/py/recompute_all_metrics.py`
  - Reported created/updated for unified amended-SSoT metric recomputation.

- `research/ab-eval/py/gen_alignment_review_html.py`
  - Iteratively modified:
    - conflict-only views,
    - switching model columns,
    - enabling/disabling JSON export per user instruction,
    - dropdown-to-button/radio interaction changes,
    - full-set review artifact variants.

- `research/ab-eval/py/run_fontclip_experiment.py`
  - Reported created for FontCLIP benefit quantification.

- `research/ab-eval/py/analyze_fontclip_delta.py`
  - Reported created for deltas/analysis.

- `research/ab-eval/py/run_agreement_experiment.py`
  - Reported created for Agreement-focused policy experiments.

- `research/ab-eval/py/mismatch_analysis.py`
  - Reported created for failure slicing/root-cause analysis.

- `research/ab-eval/py/intervention_runner.py`
  - Reported created for intervention matrix execution.

- `research/ab-eval/py/verify_pony_alpha.py`
  - Reported created to verify multimodal eligibility for `openrouter/pony-alpha`.

- `research/ab-eval/py/run_phase2_comparisons.py`
  - Reported created for requested phase 2 model run block.

- `research/ab-eval/py/compare_week1_prompt_ab.py`
  - Reported created for Week 1 control/treatment comparator.

- `research/ab-eval/py/week3_threshold_calibration.py`
  - Reported created for Week 3 threshold experiments.

- Governance lock scripts/docs reported updated/created:
  - `research/ab-eval/py/validate_gates.py` (new checker)
  - `research/ab-eval/py/score_all_variants.py` (helps/hurts outputs)
  - `research/ab-eval/py/run_all.py` (`--seed`, `--repeats`)
  - `research/ab-eval/py/gen_font_descriptions.py` (`--seed`, `--repeats`)
  - `research/ab-eval/EVALUATION_CONTRACT.md` (new canonical contract)
  - `research/ab-eval/REPRODUCIBILITY.md` (new reproducibility/quota rules)

#### C) Reports and artifacts reported created/updated

- Track A reports/artifacts:
  - `research/ab-eval/TRACK_A_REVIEW_PACKET.md`
  - `research/ab-eval/TRACK_A_RUNBOOK.md`
  - `research/ab-eval/out/test_v2_real.jsonl`

- Alignment reports/artifacts:
  - `research/ab-eval/REPORT_ALIGNMENT_SPOTCHECK.md`
  - `research/ab-eval/REPORT_ALIGNMENT_SPOTCHECK_V2.md`
  - `research/ab-eval/out/spot_check_alignment*.json`
  - `research/ab-eval/out/alignment_conflict_review.html`

- Comprehensive and no-bias rounds:
  - `research/ab-eval/REPORT_COMPREHENSIVE_235B.md`
  - `research/ab-eval/out/comprehensive_235b_results.json`
  - `research/ab-eval/REPORT_NO_BIAS_FULL_SET.md`
  - `research/ab-eval/out/full_set_no_bias_*.json`
  - `research/ab-eval/comprehensive_model_comparison_report.md`

- FontCLIP/Agreement/Failure sprint:
  - `research/ab-eval/fontclip_benefit_report.md`
  - `research/ab-eval/out/experiment_fontclip_results.json`
  - `research/ab-eval/REPORT_AGREEMENT_OPTIMIZATION.md`
  - `research/ab-eval/out/agreement_experiment_results.json`
  - `research/ab-eval/REPORT_FAILURE_ANALYSIS_SPRINT.md`

- Phase 2 comparison set:
  - `research/ab-eval/REPORT_PHASE2_COMPARISON.md`
  - `research/ab-eval/out/g3_pro_v3_gated_raw.json`
  - `research/ab-eval/out/gpt52_v3_gated_raw.json`
  - `research/ab-eval/out/metrics_g3_pro_v3_gated_raw.json`
  - `research/ab-eval/out/metrics_gpt52_v3_gated_raw.json`

- Week-based reports:
  - `research/ab-eval/REPORT_WEEK1_PROMPT_V4_AB.md`
  - `research/ab-eval/QA_SPECIMEN_V3_1.md`
  - `research/ab-eval/REPORT_WEEK2_SPECIMEN_V3_1.md`
  - `research/ab-eval/REPORT_WEEK3_CALIBRATION.md`
  - `research/ab-eval/out/week1_*`
  - `research/ab-eval/out/week2_*`
  - `research/ab-eval/out/week3_*`

- Research syntheses:
  - `research/ab-eval/REPORT_PHASE3A_LOCAL_RESEARCH.md`
  - `research/ab-eval/out/research_summary_v3a.json`
  - `research/ab-eval/REPORT_PHASE3B_ONLINE_RESEARCH.md`
  - `research/ab-eval/out/online_research_summary.json`

- User-reviewed SSoT export source used in later runs:
  - `research/ab-eval/out/full_set_review_export_1770612809775.json`

#### D) Specimen files explicitly discussed/validated by user requirement

- `research/ab-eval/out/specimens_v3/Red_Hat_Mono_top.png`
- `research/ab-eval/out/specimens_v3/Red_Hat_Mono_bottom.png`
- `research/ab-eval/out/specimens_v3/Playwrite_BE_WAL_Guides_top.png`
- `research/ab-eval/out/specimens_v3/Playwrite_BE_WAL_Guides_bottom.png`
- Regenerated/checked counterparts under:
  - `research/ab-eval/out/specimens_v3_1/`

#### E) Critical command patterns used repeatedly

```powershell
# full-set style run (example)
.\.venv-ab-eval\Scripts\python research/ab-eval/py/run_full_comparison.py --model <model> --output <artifact>.json

# production-like trial run (example)
.\.venv-ab-eval\Scripts\python research/ab-eval/py/run_production_trial.py --model gemini-3-pro-preview --prompt v3 --output <artifact>.json

# compare two run outputs
.\.venv-ab-eval\Scripts\python research/ab-eval/py/compare_week1_prompt_ab.py --control <control>.json --treatment <treatment>.json --output <comparison>.json

# governance gate check
python research/ab-eval/py/validate_gates.py research/ab-eval/out/report_all.json --out research/ab-eval/out/gate_results.json
```

---

### 5) Problem Solving State

#### Solved / addressed

- Render defects (crop/overlap) were reported fixed in V3 and verified again in V3.1 by manual QA.
- Model benchmark breadth expanded across Gemini/Qwen/GPT/Kimi variants over multiple rounds.
- No-name-bias concern explicitly addressed in full comparison path.
- Prompt and specimen interventions were tested with quantitative reporting.
- Governance ambiguity reduced by introducing a contract + gate checker + reproducibility doc (reported complete).

#### Important results snapshots (reported)

- `gemini-3-flash-preview` on amended SSoT previously reported around:
  - Agreement 0.6397, Precision 0.6415, Recall 0.7556, F1 0.6939.
- `moonshotai/kimi-k2.5` reported:
  - Agreement 0.6235, Precision 0.5932, Recall 0.7778, F1 0.6731.
- Phase 2 V3-gated comparison reported:
  - Gemini 3 Pro Preview slightly above Gemini 3 Flash on Agreement/F1,
  - GPT-5.2 extremely conservative (very high precision, very low recall).
- Week 1 A/B reported:
  - Prompt V4 did not beat V3 on Agreement/F1 under chosen policy.
- Week 2 reported:
  - Specimen V3.1 no major regression, stable Agreement and slight balance improvements.
- Week 3 reported:
  - threshold tuning did not outperform existing fixed gate on Agreement.

#### Ongoing uncertainty / caveats

- Some numerical outcomes across phases used different denominator conventions due to label `2` handling before governance lock; compare older numbers cautiously.
- Delegate-reported file changes and metrics were not always independently re-opened in parent after each subtask.
- Quota exhaustion/rate limits can alter run completeness unless key strategy is actively managed.

#### Validation quality state

- Manual QA explicitly required and reported for named specimen regressions.
- Full-set runs repeatedly executed with metrics + confusion outputs.
- Governance layer now reported to enforce promotion gates and schema checks.

---

### 6) Pending Tasks and Next Steps

#### Verbatim last instruction before this handoff request

> "Lock a single evaluat....
> Add hard acceptance gates...
> Standardize reprod....
>
> do all of these now."

#### What was done for that instruction (reported)

- Canonical contract and reproducibility docs were created/updated.
- Gate checker script was added.
- LightSpec governance decision/progress were updated.

#### Immediate next action (recommended)

1. **Perform a quick verification pass** of the governance lock artifacts to ensure they exactly match intended policy (especially label `2` rule and tie-break order).
   - Open and verify:
     - `research/ab-eval/EVALUATION_CONTRACT.md`
     - `research/ab-eval/REPRODUCIBILITY.md`
     - `research/ab-eval/py/validate_gates.py`
2. **Run one governance smoke check** on a known report artifact:
   - `python research/ab-eval/py/validate_gates.py research/ab-eval/out/report_all.json --out research/ab-eval/out/gate_results.json`
3. **If governance verification passes**, proceed to next experiment cycle under locked contract:
   - one intervention at a time,
   - promotion only if gate checks pass,
   - maintain required visual QA checklist for specimen-affecting changes.

#### Recommended roadmap after governance lock

- Short-term:
  - Confirm champion stack under newly locked contract:
    - model `gemini-3-pro-preview`,
    - prompt V3,
    - specimen V3.1,
    - gate 0.90 (as reported from Week sequence).
- Next improvements (contract-compliant):
  - targeted false-negative reduction without precision collapse,
  - query-family-aware policy only if it improves Agreement under fixed protocol,
  - specimen micro-tell enhancements only with mandatory regression QA.

#### Handoff risks

- **Environment keys**:
  - `GEMINI_API_KEY`, `OPENROUTER_API_KEY` availability and quota status are critical.
- **Quota/rate limits**:
  - long Pro runs can hit 429/RESOURCE_EXHAUSTED.
- **Denominator drift risk**:
  - any accidental deviation from locked non-binary label handling (`2` policy) invalidates comparability.
- **Artifact naming collisions**:
  - ensure strict phase prefixes (`week1_`, `week2_`, `week3_`) and canonical baselines.
- **Execution sequence risk**:
  - specimen changes must be followed by required visual QA before model reruns.

---

## Session Status Snapshot

- Active mode: orchestrator.
- Scope completion before handoff: reported complete for Week 1–3 and governance lock.
- Recommended next checkpoint: governance artifact verification smoke test + contract-compliant next experiment kickoff.
